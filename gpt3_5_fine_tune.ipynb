{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YunDcnyU5pI7"
      },
      "outputs": [],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        ""
      ],
      "metadata": {
        "id": "QgJp_Sz852W8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "input_file = 'hugging_face_train.csv'\n",
        "train_output_file = '3.5_train.csv'\n",
        "test_output_file = '3.5_test.csv'\n",
        "\n",
        "# Step 1: Read all lines from the input file\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Step 2: Randomly select 50,000 lines for training\n",
        "train_lines = random.sample(lines, 50000)\n",
        "\n",
        "# Step 3: Remove the selected training lines from the list of all lines\n",
        "remaining_lines = list(set(lines) - set(train_lines))\n",
        "\n",
        "# Step 4: Randomly select 2,500 lines for testing\n",
        "test_lines = random.sample(remaining_lines, 2500)\n",
        "\n",
        "# Step 5: Write the selected lines to their respective output files\n",
        "with open(train_output_file, 'w') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "with open(test_output_file, 'w') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "print(f\"Random 50,000 lines written to {train_output_file}\")\n",
        "print(f\"Random 2,500 lines written to {test_output_file}\")\n",
        "\n",
        "# Step 6: Download the files to your local machine\n",
        "files.download(train_output_file)\n",
        "files.download(test_output_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Kvg0PTDCCh_b",
        "outputId": "740d8ad2-7cec-470c-9c23-2de4fbec6ec3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random 50,000 lines written to 3.5_train.csv\n",
            "Random 2,500 lines written to 3.5_test.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b664a404-ea59-420d-a841-d4ad432092d5\", \"3.5_train.csv\", 1998835)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4db954c1-8666-4966-b472-a0e2415e6d81\", \"3.5_test.csv\", 99760)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "\n",
        "import openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWpPVo0pIcKn",
        "outputId": "52bd6028-fb6a-412f-df40-982cc6822101"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "0DIKeKC9lkjV",
        "outputId": "7823d2e3-ae01-43e5-f87f-3fd984f31ef3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.30.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'sk-proj-8oLvnNGJLnlgW4SQOoHwT3BlbkFJ8c24SWE59CoO4sTxlDC7'"
      ],
      "metadata": {
        "id": "kQcG318endwD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin the training\n",
        "\n",
        "\n",
        "def upload_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = openai.File.create(\n",
        "            file=f,\n",
        "            purpose='fine-tune'\n",
        "        )\n",
        "    return response['id']\n",
        "\n",
        "dataset_path = 'gpt_3.5_train.txt'\n",
        "file_id = upload_file(dataset_path)\n",
        "print(f\"Uploaded file ID: {file_id}\")\n",
        "\n",
        "def create_fine_tune_job(file_id, model='gpt-3.5-turbo'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "fine_tune_response = create_fine_tune_job(file_id)\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "import time\n",
        "\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTune.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(60)\n",
        "\n",
        "fine_tuned_model = openai.FineTune.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)\n",
        "\n",
        "print(\"done\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IyGNOgDMCIaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai\n"
      ],
      "metadata": {
        "id": "ml-vKwbtn3WR",
        "outputId": "3b1f5234-9b81-4c48-90a7-93b300bb95ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.30.4-py3-none-any.whl (320 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.30.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "import openai"
      ],
      "metadata": {
        "id": "Y5hNe9TurMAP",
        "outputId": "c6b6b5eb-b88b-4069-ff2c-1752d9ae0c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "KQAXPKoMsVBX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file = 'gpt_3.5_train.txt'\n",
        "output_file = 'formatted_dataset.jsonl'\n",
        "\n",
        "# Step 1: Read the input file\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Step 2: Format each line into a JSON object\n",
        "json_lines = []\n",
        "for line in lines:\n",
        "    # Split the line into clue, length, and answer\n",
        "    parts = line.strip().split(', ')\n",
        "    if len(parts) == 3:\n",
        "        clue, length, answer = parts\n",
        "        # Create a JSON object\n",
        "        json_obj = {\n",
        "            \"prompt\": f\"{clue}, {length}\",\n",
        "            \"completion\": answer.strip()\n",
        "        }\n",
        "        json_lines.append(json_obj)\n",
        "\n",
        "# Step 3: Write the JSON objects to a JSONL file\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for json_obj in json_lines:\n",
        "        outfile.write(json.dumps(json_obj) + '\\n')\n",
        "\n",
        "print(f\"Converted {len(lines)} lines to JSONL format and saved to {output_file}\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "id": "XddqpSZs-fUA",
        "outputId": "c5cbbcd4-2601-451e-fdcd-cc457abcfb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50000 lines to JSONL format and saved to formatted_dataset.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_55215a02-a51b-4af1-a735-8e54cf914342\", \"formatted_dataset.jsonl\", 2636175)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Check the JSONL file format\n",
        "output_file = 'formatted_dataset.jsonl'\n",
        "with open(output_file, 'r') as infile:\n",
        "    for line in infile:\n",
        "        try:\n",
        "            json_obj = json.loads(line)\n",
        "            if 'prompt' not in json_obj or 'completion' not in json_obj:\n",
        "                print(f\"Invalid format: {line}\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"JSON decode error: {line}\")\n",
        "print(\"All good\")"
      ],
      "metadata": {
        "id": "gsKQ6YEDAthP",
        "outputId": "e47a950c-d0ec-42f7-9646-826bbe2f29ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "KfzRH40B_RCb",
        "outputId": "020bdad9-c5ed-4a33-bc8e-858f50e3f48e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "id": "8pHI3ke3_sEj",
        "outputId": "371bc73e-4422-4734-ceea-a64b0c533021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file = 'gpt_3.5_train.txt'\n",
        "output_file = 'chat_formatted_dataset.jsonl'\n",
        "\n",
        "# Read the input file\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Format each line into a JSON object for chat\n",
        "json_lines = []\n",
        "for line in lines:\n",
        "    parts = line.strip().split(', ')\n",
        "    if len(parts) == 3:\n",
        "        clue, length, answer = parts\n",
        "        json_obj = {\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"{clue}, {length}\"},\n",
        "                {\"role\": \"assistant\", \"content\": answer.strip()}\n",
        "            ]\n",
        "        }\n",
        "        json_lines.append(json_obj)\n",
        "\n",
        "# Write the JSON objects to a JSONL file\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for json_obj in json_lines:\n",
        "        outfile.write(json.dumps(json_obj) + '\\n')\n",
        "\n",
        "print(f\"Converted {len(lines)} lines to chat format and saved to {output_file}\")\n",
        "\n",
        "# Download the file to your local machine\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "id": "1VSHhZWAArCx",
        "outputId": "8cb566c7-363e-4f12-914b-c2b7a86afac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 50000 lines to chat format and saved to chat_formatted_dataset.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7f734812-d35d-46a3-bda3-21d68a496936\", \"chat_formatted_dataset.jsonl\", 7857219)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'sk-proj-8oLvnNGJLnlgW4SQOoHwT3BlbkFJ8c24SWE59CoO4sTxlDC7'"
      ],
      "metadata": {
        "id": "oL66tq3e_7GA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "input_file = 'chat_formatted_dataset.jsonl'\n",
        "output_file = 'chat_formatted_dataset_2000.jsonl'\n",
        "\n",
        "# Read the input JSONL file\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Ensure there are at least 2000 lines\n",
        "if len(lines) < 2000:\n",
        "    raise ValueError(\"The input file contains fewer than 2000 lines.\")\n",
        "\n",
        "# Randomly select 2000 lines\n",
        "selected_lines = random.sample(lines, 2000)\n",
        "\n",
        "# Write the selected lines to the output JSONL file\n",
        "with open(output_file, 'w') as outfile:\n",
        "    for line in selected_lines:\n",
        "        outfile.write(line)\n",
        "\n",
        "print(f\"Selected 2000 lines and saved to {output_file}\")\n",
        "\n",
        "# Download the file to your local machine\n",
        "from google.colab import files\n",
        "files.download(output_file)\n"
      ],
      "metadata": {
        "id": "78YumdBIGuFP",
        "outputId": "6a198460-aa76-4ccf-cd87-71356d411090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 2000 lines and saved to chat_formatted_dataset_2000.jsonl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bba63160-6459-4ec5-85b1-ebb49985f32d\", \"chat_formatted_dataset_2000.jsonl\", 349481)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output_file = 'chat_formatted_dataset_2000.jsonl'\n",
        "\n",
        "# Step 2: Upload the dataset\n",
        "def upload_file(file_path):\n",
        "    response = openai.File.create(\n",
        "        file=open(file_path, \"rb\"),\n",
        "        purpose='fine-tune'\n",
        "    )\n",
        "    return response['id']\n",
        "\n",
        "train_file_id = upload_file(train_output_file)\n",
        "print(f\"Uploaded train file ID: {train_file_id}\")\n",
        "\n",
        "# Step 3: Create a fine-tuning job\n",
        "def create_fine_tune_job(training_file_id, model='gpt-3.5-turbo'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=training_file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "# Use the correct endpoint for creating fine-tunes\n",
        "fine_tune_response = openai.FineTuningJob.create(training_file=train_file_id, model='gpt-3.5-turbo')\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "def get_fine_tune_job_details(job_id):\n",
        "    response = openai.FineTuningJob.retrieve(id=job_id)\n",
        "    return response\n",
        "\n",
        "\n",
        "# Step 4: Monitor the fine-tuning job\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTuningJob.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "print(job_id)\n",
        "\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    #details = get_fine_tune_job_details(job_id)\n",
        "    #print(f\"Details: {details}\")\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(45)\n",
        "\n",
        "# Step 5: Save the fine-tuned model name\n",
        "fine_tuned_model = openai.FineTuningJob.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "# Save the fine-tuned model name to a file\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)"
      ],
      "metadata": {
        "id": "zCzwARI5nbBe",
        "outputId": "da9ba5b7-c2f8-42c4-dd93-62b4fb026f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded train file ID: file-crnSMWCLbtyChDJJW1Mha3uq\n",
            "Fine-tune job created: ftjob-z57JP0WeZUuwIkTShHCR5qWF\n",
            "ftjob-z57JP0WeZUuwIkTShHCR5qWF\n",
            "Monitoring fine-tuning job...\n",
            "Job status: validating_files\n",
            "Job status: validating_files\n",
            "Job status: validating_files\n",
            "Job status: validating_files\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: running\n",
            "Job status: succeeded\n",
            "Fine-tuning job completed successfully!\n",
            "Fine-tuned model name: ft:gpt-3.5-turbo-0125:personal::9ULF3f43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "import openai\n",
        "\n",
        "openai.api_key = 'sk-proj-8oLvnNGJLnlgW4SQOoHwT3BlbkFJ8c24SWE59CoO4sTxlDC7'"
      ],
      "metadata": {
        "id": "P54ycfv3e91o",
        "outputId": "4cb36935-ad2f-4337-ebb9-0a4a448ca20e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "id": "qt-BFSYtfHoC",
        "outputId": "3d834839-559a-4e39-9304-aeeabf2e6188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Step 3: Load the fine-tuned model name from the file\n",
        "with open('fine_tuned_model_name.txt', 'r') as f:\n",
        "    fine_tuned_model = f.read().strip()"
      ],
      "metadata": {
        "id": "W7VYh9uIgnu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate a completion using the fine-tuned model\n",
        "def generate_completion(prompt, model):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].message['content'].strip()\n",
        "\n",
        "\n",
        "def generate_multiple_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    responses = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    completions = [response['text'].strip() for response in responses['choices']]\n",
        "    return completions\n",
        "\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completions = generate_multiple_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Use the fine-tuned model\n",
        "print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "prompt = \"Some crumbly blocks, 4\"\n",
        "completion = generate_completion(prompt, fine_tuned_model)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Completion: {completion}\")\n"
      ],
      "metadata": {
        "id": "rSGDzJZmf4Yl",
        "outputId": "1e0ba667-1953-42f7-9736-e76004fd986c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidRequestError",
          "evalue": "Missing required parameter: 'messages'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-db9f0bc79232>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Fennel or sweet cicely\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_multiple_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfine_tuned_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_completions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completion {i}: {completion}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-db9f0bc79232>\u001b[0m in \u001b[0;36mgenerate_multiple_completions\u001b[0;34m(prompt, model, num_completions, max_tokens)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_multiple_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_completions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     responses = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mInvalidRequestError\u001b[0m: Missing required parameter: 'messages'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_unique_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions * 2,  # Request more completions to increase chances of uniqueness\n",
        "        stop=None,\n",
        "        temperature=0.9,  # Increase temperature for more diverse answers\n",
        "        top_p=0.9  # Use top-p sampling for diversity\n",
        "    )\n",
        "    completions = set()\n",
        "    for choice in response['choices']:\n",
        "        completions.add(choice['message']['content'].strip())\n",
        "        if len(completions) >= num_completions:\n",
        "            break\n",
        "\n",
        "    return list(completions)[:num_completions]  # Return only the requested number of unique completions\n",
        "\n",
        "# Example usage\n",
        "prompt = \"Tries for a role, 5, R___S\"\n",
        "completions = generate_unique_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n"
      ],
      "metadata": {
        "id": "5S_0DU7ChH_7",
        "outputId": "d4695e2a-45a5-4096-a1de-2650d40edd34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion 1: READS:^{\n",
            "Completion 2: READS|--------------------------------------------------------------------------\n",
            "Completion 3: READS\n",
            "Completion 4: AUDITS\n",
            "Completion 5: READSF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7wipz6_tXi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "import openai"
      ],
      "metadata": {
        "id": "JrcrxYKNmQne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai.api_key = 'sk-proj-04xTZxjLUmsxc05S1jlNT3BlbkFJ1pYCgmK5xeM0vB9YqlJR'\n",
        "\n",
        "\n",
        "def upload_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = openai.File.create(\n",
        "            file=f,\n",
        "            purpose='fine-tune'\n",
        "        )\n",
        "    return response['id']\n",
        "\n",
        "dataset_path = 'gpt_3.5_train.txt'\n",
        "file_id = upload_file(dataset_path)\n",
        "print(f\"Uploaded file ID: {file_id}\")\n",
        "\n",
        "# Step 3: Create a fine-tuning job\n",
        "def create_fine_tune_job(file_id, model='gpt-3.5-turbo'):\n",
        "    response = openai.FineTune.create(\n",
        "        training_file=file_id,\n",
        "        model=model\n",
        "    )\n",
        "    return response\n",
        "\n",
        "fine_tune_response = create_fine_tune_job(train_file_id)\n",
        "print(f\"Fine-tune job created: {fine_tune_response['id']}\")\n",
        "\n",
        "# Step 4: Monitor the fine-tuning job\n",
        "def check_fine_tune_status(job_id):\n",
        "    response = openai.FineTune.retrieve(id=job_id)\n",
        "    status = response['status']\n",
        "    return status\n",
        "\n",
        "job_id = fine_tune_response['id']\n",
        "\n",
        "print(\"Monitoring fine-tuning job...\")\n",
        "while True:\n",
        "    status = check_fine_tune_status(job_id)\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status == 'succeeded':\n",
        "        print(\"Fine-tuning job completed successfully!\")\n",
        "        break\n",
        "    elif status == 'failed':\n",
        "        print(\"Fine-tuning job failed.\")\n",
        "        break\n",
        "    time.sleep(60)  # Check status every minute\n",
        "\n",
        "# Step 5: Save the fine-tuned model name\n",
        "fine_tuned_model = openai.FineTune.retrieve(id=job_id)['fine_tuned_model']\n",
        "print(f\"Fine-tuned model name: {fine_tuned_model}\")\n",
        "\n",
        "# Save the fine-tuned model name to a file\n",
        "with open('fine_tuned_model_name.txt', 'w') as f:\n",
        "    f.write(fine_tuned_model)"
      ],
      "metadata": {
        "id": "srlGWNpRl0iD",
        "outputId": "0f220be1-b8ad-4fc2-c86c-7310b4a5ec21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploaded file ID: file-tXI1N4xwQgDm6wDJZAE6TQie\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_file_id' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-99b17153746d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mfine_tune_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_fine_tune_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fine-tune job created: {fine_tune_response['id']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_file_id' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential Load???? Figure this out?\n",
        "\n",
        "# Step 1: Install the OpenAI library\n",
        "!pip install openai\n",
        "\n",
        "# Step 2: Import required libraries and set up the OpenAI API key\n",
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'YOUR_API_KEY'\n",
        "\n",
        "# Step 3: Load the fine-tuned model name from the file\n",
        "with open('fine_tuned_model_name.txt', 'r') as f:\n",
        "    fine_tuned_model = f.read().strip()\n",
        "\n",
        "# Step 4: Generate a completion using the fine-tuned model\n",
        "def generate_completion(prompt, model):\n",
        "    response = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=50\n",
        "    )\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Use the fine-tuned model\n",
        "print(f\"Using fine-tuned model: {fine_tuned_model}\")\n",
        "\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completion = generate_completion(prompt, fine_tuned_model)\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Completion: {completion}\")\n"
      ],
      "metadata": {
        "id": "ZdDyxnt7lQll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_completions(prompt, model, num_completions=5, max_tokens=50):\n",
        "    responses = openai.Completion.create(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        n=num_completions,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    completions = [response['text'].strip() for response in responses['choices']]\n",
        "    return completions\n",
        "\n",
        "prompt = \"Fennel or sweet cicely\"\n",
        "completions = generate_multiple_completions(prompt, fine_tuned_model, num_completions=5)\n",
        "for i, completion in enumerate(completions, 1):\n",
        "    print(f\"Completion {i}: {completion}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "x29D6Xvjitrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "input_file = 'reformatted.txt'\n",
        "train_output_file = 'gpt_3.5_train.txt'\n",
        "test_output_file = 'gpt_3.5_test.txt'\n",
        "\n",
        "# Step 1: Read all lines from the input file\n",
        "with open(input_file, 'r') as infile:\n",
        "    lines = infile.readlines()\n",
        "\n",
        "# Step 2: Randomly select 50,000 lines for training\n",
        "train_lines = random.sample(lines, 50000)\n",
        "\n",
        "# Step 3: Remove the selected training lines from the list of all lines\n",
        "remaining_lines = list(set(lines) - set(train_lines))\n",
        "\n",
        "# Step 4: Randomly select 2,500 lines for testing\n",
        "test_lines = random.sample(remaining_lines, 2500)\n",
        "\n",
        "# Step 5: Write the selected lines to their respective output files\n",
        "with open(train_output_file, 'w') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "with open(test_output_file, 'w') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "print(f\"Random 50,000 lines written to {train_output_file}\")\n",
        "print(f\"Random 2,500 lines written to {test_output_file}\")\n",
        "\n",
        "# Step 6: Download the files to your local machine\n",
        "files.download(train_output_file)\n",
        "files.download(test_output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "P3F9bIFYGg7y",
        "outputId": "598184d2-4623-42d1-9df7-ed95d11ee3b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random 50,000 lines written to gpt_3.5_train.txt\n",
            "Random 2,500 lines written to gpt_3.5_test.txt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4fba1d17-9038-47d6-b1f3-8d44bcb0621c\", \"gpt_3.5_train.txt\", 1459214)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_38a6c34a-c1d3-4261-a9ea-0843980c0adb\", \"gpt_3.5_test.txt\", 77936)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential Loopy Belief Code\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Example nested dictionary input\n",
        "clues = {\n",
        "    \"clue1\": {\"SKY\": 0.2, \"FLY\": 0.5, \"HEY\": 0.3},\n",
        "    \"clue2\": {\"HI\": 0.4, \"OH\": 0.6},\n",
        "    # Add more clues\n",
        "}\n",
        "\n",
        "# Example grid format (coordinates for each clue)\n",
        "grid = {\n",
        "    \"clue1\": [(0, 0), (0, 1), (0, 2)],\n",
        "    \"clue2\": [(0, 2), (1, 2)],\n",
        "    # Add more clues with their respective coordinates\n",
        "}\n",
        "\n",
        "# Initialize messages\n",
        "messages = {clue: {neighbor: np.ones(len(possible_answers)) / len(possible_answers)\n",
        "                   for neighbor in grid if neighbor != clue}\n",
        "            for clue, possible_answers in clues.items()}\n",
        "\n",
        "# Update messages (loopy belief propagation)\n",
        "def update_messages(clues, grid, messages, iterations=10):\n",
        "    for _ in range(iterations):\n",
        "        new_messages = messages.copy()\n",
        "        for clue, neighbors in grid.items():\n",
        "            for neighbor in neighbors:\n",
        "                for possible_answer, prob in clues[clue].items():\n",
        "                    message_product = 1\n",
        "                    for other_neighbor in grid[clue]:\n",
        "                        if other_neighbor != neighbor:\n",
        "                            message_product *= messages[other_neighbor][clue][possible_answer]\n",
        "                    new_messages[clue][neighbor][possible_answer] = prob * message_product\n",
        "                new_messages[clue][neighbor] /= new_messages[clue][neighbor].sum()\n",
        "        messages = new_messages\n",
        "    return messages\n",
        "\n",
        "# Calculate beliefs\n",
        "def calculate_beliefs(clues, grid, messages):\n",
        "    beliefs = {}\n",
        "    for clue, possible_answers in clues.items():\n",
        "        belief = np.ones(len(possible_answers))\n",
        "        for neighbor in grid[clue]:\n",
        "            belief *= messages[neighbor][clue]\n",
        "        belief *= np.array(list(possible_answers.values()))\n",
        "        beliefs[clue] = belief / belief.sum()\n",
        "    return beliefs\n",
        "\n",
        "# Run loopy belief propagation\n",
        "messages = update_messages(clues, grid, messages)\n",
        "beliefs = calculate_beliefs(clues, grid, messages)\n",
        "\n",
        "# Determine final answers\n",
        "final_answers = {clue: max(possible_answers, key=lambda ans: beliefs[clue][ans_idx])\n",
        "                 for clue, possible_answers in clues.items()\n",
        "                 for ans_idx, ans in enumerate(possible_answers)}\n",
        "\n",
        "print(\"Final Answers:\", final_answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "bGx4QVmiIIm3",
        "outputId": "c6974e97-7a08-477b-ed0e-49576d3d8638"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "(0, 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b645a57514e5>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Run loopy belief propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0mbeliefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_beliefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b645a57514e5>\u001b[0m in \u001b[0;36mupdate_messages\u001b[0;34m(clues, grid, messages, iterations)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mother_neighbor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mother_neighbor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                             \u001b[0mmessage_product\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_neighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossible_answer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossible_answer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmessage_product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example nested dictionary input\n",
        "clues = {\n",
        "    \"clue1\": {\"SKY\": 0.2, \"FLY\": 0.5, \"BRO\": 0.3},\n",
        "    \"clue2\": {\"HI\": 0.4, \"OH\": 0.6},\n",
        "    # Add more clues\n",
        "}\n",
        "\n",
        "# Example grid format (coordinates for each clue)\n",
        "grid = {\n",
        "    \"clue1\": [(0, 0), (0, 1), (0, 2)],\n",
        "    \"clue2\": [(0, 2), (1, 2)],\n",
        "    # Add more clues with their respective coordinates\n",
        "}\n",
        "\n",
        "# Find neighbors based on grid intersections\n",
        "def find_neighbors(grid):\n",
        "    coord_to_clues = {}\n",
        "    for clue, coords in grid.items():\n",
        "        for coord in coords:\n",
        "            if coord not in coord_to_clues:\n",
        "                coord_to_clues[coord] = []\n",
        "            coord_to_clues[coord].append(clue)\n",
        "\n",
        "    neighbors = {clue: set() for clue in grid}\n",
        "    for coord, clues_at_coord in coord_to_clues.items():\n",
        "        for clue in clues_at_coord:\n",
        "            neighbors[clue].update(clues_at_coord)\n",
        "            neighbors[clue].remove(clue)\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "# Get neighbors for each clue\n",
        "neighbors = find_neighbors(grid)\n",
        "\n",
        "# Initialize messages\n",
        "messages = {clue: {neighbor: np.ones(len(clues[neighbor])) / len(clues[neighbor])\n",
        "                   for neighbor in neighbors[clue]}\n",
        "            for clue in clues}\n",
        "\n",
        "# Print initialized messages\n",
        "print(\"Initialized messages:\")\n",
        "for clue, msgs in messages.items():\n",
        "    print(f\"{clue}:\")\n",
        "    for neighbor, msg in msgs.items():\n",
        "        print(f\"  to {neighbor}: {msg}\")\n",
        "\n",
        "# Example function to update messages (simplified for illustration)\n",
        "def update_messages(clues, neighbors, messages, iterations=10):\n",
        "    for _ in range(iterations):\n",
        "        new_messages = {clue: {neighbor: np.ones(len(clues[neighbor])) / len(clues[neighbor])\n",
        "                               for neighbor in neighbors[clue]}\n",
        "                        for clue in clues}\n",
        "        for clue in clues:\n",
        "            for neighbor in neighbors[clue]:\n",
        "                for possible_answer, prob in clues[clue].items():\n",
        "                    message_product = 1\n",
        "                    for other_neighbor in neighbors[clue]:\n",
        "                        if other_neighbor != neighbor:\n",
        "                            message_product *= messages[other_neighbor][clue][possible_answer]\n",
        "                    new_messages[clue][neighbor][possible_answer] = prob * message_product\n",
        "                new_messages[clue][neighbor] /= new_messages[clue][neighbor].sum()\n",
        "        messages = new_messages\n",
        "    return messages\n",
        "\n",
        "# Run loopy belief propagation\n",
        "messages = update_messages(clues, neighbors, messages)\n",
        "\n",
        "# Example function to calculate beliefs\n",
        "def calculate_beliefs(clues, neighbors, messages):\n",
        "    beliefs = {}\n",
        "    for clue, possible_answers in clues.items():\n",
        "        belief = np.ones(len(possible_answers))\n",
        "        for neighbor in neighbors[clue]:\n",
        "            belief *= messages[neighbor][clue]\n",
        "        belief *= np.array(list(possible_answers.values()))\n",
        "        beliefs[clue] = belief / belief.sum()\n",
        "    return beliefs\n",
        "\n",
        "# Calculate beliefs\n",
        "beliefs = calculate_beliefs(clues, neighbors, messages)\n",
        "\n",
        "# Determine final answers\n",
        "final_answers = {clue: max(clues[clue], key=lambda ans: beliefs[clue][list(clues[clue].keys()).index(ans)])\n",
        "                 for clue in clues}\n",
        "\n",
        "print(\"Final Answers:\", final_answers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "IGtn9df0-xeR",
        "outputId": "e63fc656-e8de-4253-c8b7-b353ef48530d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized messages:\n",
            "clue1:\n",
            "  to clue2: [0.5 0.5]\n",
            "clue2:\n",
            "  to clue1: [0.33333333 0.33333333 0.33333333]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-5637656fb259>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Run loopy belief propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;31m# Example function to calculate beliefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-5637656fb259>\u001b[0m in \u001b[0;36mupdate_messages\u001b[0;34m(clues, neighbors, messages, iterations)\u001b[0m\n\u001b[1;32m     60\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mother_neighbor\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mneighbor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                             \u001b[0mmessage_product\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_neighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossible_answer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossible_answer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmessage_product\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnew_messages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    }
  ]
}