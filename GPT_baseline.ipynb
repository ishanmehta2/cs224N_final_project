{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chardet\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "filepath = '/Users/andrewlipschultz/Dropbox/My Mac (Andrewâ€™s MacBook Pro)/Documents/GitHub/cs224N_final_project/nytcrosswords.csv'\n",
    "filepath = 'nytcrosswords.csv'\n",
    "with open(filepath, 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "    encoding = result['encoding']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Word                                 Clue\n",
      "0  10/31/2021      PAT  Action done while saying \"Good dog\"\n",
      "1  10/31/2021  RASCALS                      Mischief-makers\n",
      "2  10/31/2021      PEN          It might click for a writer\n",
      "3  10/31/2021      SEP                             Fall mo.\n",
      "4  10/31/2021      ECO                Kind to Mother Nature\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(filepath, encoding=encoding)\n",
    "print(df.head())\n",
    "\n",
    "relevant_cols = ['Clue', 'Word']\n",
    "for col in relevant_cols:\n",
    "    text_data = df[col].apply(str).tolist()\n",
    "    training_text = \"\\n\".join(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe064429abbd4491afbf5d223a0858dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb05d830b31e4627b94087846b595820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('training_data.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(training_text)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewlipschultz/miniconda3/envs/cs224n/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='training_data.txt',\n",
    "    block_size=128\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are currently 17,458 private web sites (listed and populated by users over the world) that use Google Analytics to conduct marketing and advertising related research and analyze customer information to optimize content.\n",
      "\n",
      "If you're interested in finding out more â€“ get to know others before jumping forward to the next phase of your research by clicking on the link!\n",
      "\n",
      "\n",
      "To see the full list of web sites by user, click here!\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "text_prompt = \"\"\n",
    "generated_text = generator(text_prompt, max_length=100, num_return_sequences=1)\n",
    "print(generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clues = []\n",
    "answers = []\n",
    "lengths = []\n",
    "with open('output.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip().strip('\"')\n",
    "        parts = line.rsplit(',', 2)\n",
    "        if len(parts) == 3:\n",
    "            clue = parts[0].strip().strip('\"')\n",
    "            answer = parts[1].strip().strip('\"')\n",
    "            length = int(parts[2].strip())\n",
    "            clues.append(clue)\n",
    "            answers.append(answer)\n",
    "            lengths.append(lengths)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    length = lengths[0]\n",
    "    clue = clues[0]\n",
    "    prompt = f\"Length of the word is: {length} Clue: {clue} Answer:\"\n",
    "    response = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    full_response = response[0]['generated_text']\n",
    "    answer = full_response.split(\"Answer:\")[1].strip().split('\\n')[0].strip()\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
